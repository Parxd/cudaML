{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Review of Pt. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: np.ndarray):\n",
    "    return np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ \\bm{x} = [x_1, x_2, x_3] $, then $ f(\\bm{x}) = [x_1^e, x_2^e, x_3^e] $.\n",
    "\n",
    "$ f(\\bm{x}): R^3 \\rightarrow R^3 $, thus the Jacobian should be of shape `3 x 3`.\n",
    "\n",
    "The Jacobian of $ f(\\bm{x}) $, $J(f)$, is the matrix:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\n",
    "{\\frac{\\partial f_1}{x_1}} & {\\frac{\\partial f_1}{x_2}} & {\\frac{\\partial f_1}{x_3}} \\\\\n",
    "{\\frac{\\partial f_2}{x_1}} & {\\frac{\\partial f_2}{x_2}} & {\\frac{\\partial f_2}{x_3}} \\\\\n",
    "{\\frac{\\partial f_3}{x_1}} & {\\frac{\\partial f_3}{x_2}} & {\\frac{\\partial f_3}{x_3}}\n",
    "\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$=\\begin{bmatrix}\n",
    "{x_1^e} & {0} & {0} \\\\\n",
    "{0} & {x_2^e} & {0} \\\\\n",
    "{0} & {0} & {x_3^e}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Like for *most* other functions, $J(f)$ represents an affine, diagonal linear mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_f(x: np.ndarray):\n",
    "    return np.eye(x.shape[0]) * np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what does the **VJP** (vector-Jacobian product) really *mean*?\n",
    "\n",
    "We want to find a sensitivity \"mapping\" to see how each of the outputs are affected by each element of the input vector $\\bm{x}$.\n",
    "\n",
    "To do this, we left multiply the transpose of our desired \"weighting\" (see Pt. 1) by $J(f)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.71828183,  7.3890561 , 20.08553692])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "f_x = f(x)\n",
    "f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71828183,  7.3890561 , 20.08553692]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.array([[1],\n",
    "              [1],\n",
    "              [1]])\n",
    "u.T @ J_f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VJP is exactly the same as $f(\\bm{x})$, which is what we'd expect.\n",
    "\n",
    "Why?\n",
    "\n",
    "- We want to find the sensitivity of $f(x_1), f(x_2), f(x_3)$ to $x_1, x_2, x_3$.\n",
    "- By setting `u` to a `1 x 3` ones-vector, we're saying that we want to see the sensitivity of all 3 outputs to the 3 inputs.\n",
    "    - Think of `u` as a \"masking\" or \"weighting\" vector for each element's sensitivity.\n",
    "- Intuitively, if $f(x_1) = x_1^e$, then any perturbation $h$ in $x_1$ will directly propagate to $f(x_1)$.\n",
    "    - $(x_1 + h)^e = f(x_1 + h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare w/ PyTorch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, requires_grad=True)\n",
    "y = x.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x))\n",
    "assert (y == x.grad).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is all in an ideal situation. In reality, it's inefficient to find Jacobians for *every* operation and compute VJPs explicitly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of element-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $f(\\bm{x}): R^n \\rightarrow R^n$ and $g(\\bm{y}): R^n \\rightarrow R^n$, where both represent an element-wise binary operation and $|\\bm{x}| = |\\bm{y}| = n$.\n",
    "\n",
    "Since input vector space must match that of output, $J(f)$ and $J(g)$ must be `n x n`.\n",
    "\n",
    "Now, letting $\\bm{z}\\ = f + g$, we want the Jacobian of $\\bm{z}$ w.r.t $\\bm{x}$ and $\\bm{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1, 3)\n",
    "y = np.random.randn(1, 3)\n",
    "\n",
    "def f(x):\n",
    "    return x + y\n",
    "\n",
    "def g(y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(f) =\n",
    "\\begin{bmatrix}\n",
    "\n",
    "{\\frac{\\partial f_1}{\\partial x_1}} & {\\frac{\\partial f_1}{\\partial x_2}} & {\\frac{\\partial f_1}{\\partial x_3}} \\\\\n",
    "{\\frac{\\partial f_2}{\\partial x_1}} & {\\frac{\\partial f_2}{\\partial x_2}} & {\\frac{\\partial f_2}{\\partial x_3}} \\\\\n",
    "{\\frac{\\partial f_3}{\\partial x_1}} & {\\frac{\\partial f_3}{\\partial x_2}} & {\\frac{\\partial f_3}{\\partial x_3}}\n",
    "\n",
    "\\end{bmatrix} = I_3\n",
    "$$\n",
    "\n",
    "$$J(g) =\n",
    "\\begin{bmatrix}\n",
    "\n",
    "{\\frac{\\partial g_1}{\\partial y_1}} & {\\frac{\\partial g_1}{\\partial y_2}} & {\\frac{\\partial g_1}{\\partial y_3}} \\\\\n",
    "{\\frac{\\partial g_2}{\\partial y_1}} & {\\frac{\\partial g_2}{\\partial y_2}} & {\\frac{\\partial g_2}{\\partial y_3}} \\\\\n",
    "{\\frac{\\partial g_3}{\\partial y_1}} & {\\frac{\\partial g_3}{\\partial y_2}} & {\\frac{\\partial g_3}{\\partial y_3}}\n",
    "\n",
    "\\end{bmatrix} = I_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of vector addition for both operands is the identity matrix.\n",
    "\n",
    "What about the Hadamard product? The idea is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${x} = \\begin{bmatrix} 1 && 2 && 3\\end{bmatrix}$$\n",
    "$${y} = \\begin{bmatrix} 4 && 5 && 6\\end{bmatrix}$$\n",
    "$${z} = {x} \\odot {y} = \\begin{bmatrix} 4 && 10 && 18 \\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial {z}}{\\partial {x}} = \\begin{bmatrix} {\\frac{\\partial z_1}{\\partial x_1}} && {\\frac{\\partial z_1}{\\partial x_2}} && {\\frac{\\partial z_1}{\\partial x_3}} \\\\\n",
    "                                                            {\\frac{\\partial z_2}{\\partial x_1}} && {\\frac{\\partial z_2}{\\partial x_2}} && {\\frac{\\partial z_2}{\\partial x_3}} \\\\\n",
    "                                                            {\\frac{\\partial z_3}{\\partial x_1}} && {\\frac{\\partial z_3}{\\partial x_2}} && {\\frac{\\partial z_3}{\\partial x_3}} \\end{bmatrix} $$\n",
    "\n",
    "$$  = \\begin{bmatrix} {\\frac{\\partial ({x} \\odot {y})_1}{\\partial x_1}} && {\\frac{\\partial ({x} \\odot {y})_1}{\\partial x_2}} && {\\frac{\\partial ({x} \\odot {y})_1}{\\partial x_3}} \\\\\n",
    "                                                   {\\frac{\\partial ({x} \\odot {y})_2}{\\partial x_1}} && {\\frac{\\partial ({x} \\odot {y})_2}{\\partial x_2}} && {\\frac{\\partial ({x} \\odot {y})_2}{\\partial x_3}} \\\\\n",
    "                                                   {\\frac{\\partial ({x} \\odot {y})_3}{\\partial x_1}} && {\\frac{\\partial ({x} \\odot {y})_3}{\\partial x_2}} && {\\frac{\\partial ({x} \\odot {y})_3}{\\partial x_3}} \\end{bmatrix} $$\n",
    "\n",
    "Again, all diagonal elements are zero'd out, since $\\frac{\\partial ({x} \\odot {y})_n}{\\partial x_m} = 0$ when $n \\ne m$.\n",
    "\n",
    "$$ = \\begin{bmatrix} \n",
    "\n",
    "\\frac{\\partial ({x} \\odot {y})_1}{\\partial x_1} && 0 && 0 \\\\\n",
    "0 && \\frac{\\partial ({x} \\odot {y})_2}{\\partial x_2} && 0 \\\\\n",
    "0 && 0 && \\frac{\\partial ({x} \\odot {y})_3}{\\partial x_3}\n",
    "\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ = \\begin{bmatrix}\n",
    "y_1 && 0 && 0 \\\\\n",
    "0 && y_2 && 0 \\\\\n",
    "0 && 0 && y_3\n",
    "\\end{bmatrix} = diag({y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the Hadamard product is another diagonal matrix. This is good, because we can avoid the cost of matmul in an automatic differentiation framework. We can simply Hadamard product the incoming gradient with the exact vector representation of the other operand, which in this case is `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6845, 0.2623, 0.2573]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 3, requires_grad=True)\n",
    "y = torch.rand(1, 3, requires_grad=True)\n",
    "z = x * y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.ones_like(z))\n",
    "assert (y == x.grad).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Sum Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the vector reduction $f: R^n \\rightarrow R$, where $f(\\bm{x}) = x_1 + x_2 + x_3 + \\dots + x_n$.\n",
    "\n",
    "What would be $J(f)$?\n",
    "- $J(f)$ would be a linear transformation from $R^n \\rightarrow R$ as well, because it is a mapping of how each element of $\\bm{x}$ affects $f$.\n",
    "- In other words, it would be a `1 x n` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial y}{\\partial \\bm{x}} = \\begin{bmatrix}\n",
    "\n",
    "    \\frac{\\partial y}{\\partial x_1} &&\n",
    "    \\frac{\\partial y}{\\partial x_2} &&\n",
    "    \\frac{\\partial y}{\\partial x_3} &&\n",
    "    \\dots && \n",
    "    \\frac{\\partial y}{\\partial x_n}\n",
    "    \n",
    "    \\end{bmatrix}$$ \n",
    "\n",
    "$$ = \\begin{bmatrix}\n",
    "\n",
    "    \\frac{\\partial \\sum_{i=1}^{n}{x_i}}{\\partial x_1} &&\n",
    "    \\frac{\\partial \\sum_{i=1}^{n}{x_i}}{\\partial x_2} &&\n",
    "    \\frac{\\partial \\sum_{i=1}^{n}{x_i}}{\\partial x_3} &&\n",
    "    \\dots && \n",
    "    \\frac{\\partial \\sum_{i=1}^{n}{x_i}}{\\partial x_n}\n",
    "    \n",
    "    \\end{bmatrix}$$ \n",
    "\n",
    "$$ = \\begin{bmatrix}\n",
    "\n",
    "    \\sum_{i=1}^{n}\\frac{\\partial {x_i}}{\\partial x_1} &&\n",
    "    \\sum_{i=1}^{n}\\frac{\\partial {x_i}}{\\partial x_2} &&\n",
    "    \\sum_{i=1}^{n}\\frac{\\partial {x_i}}{\\partial x_3} &&\n",
    "    \\dots && \n",
    "    \\sum_{i=1}^{n}\\frac{\\partial {x_i}}{\\partial x_n}\n",
    "    \n",
    "    \\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general term $\\sum_{i=1}^{n}\\frac{\\partial {x_i}}{\\partial x_n} = 1$, since when $i \\ne n$, $\\frac{\\partial {x_i}}{\\partial x_n} = 0$.\n",
    "\n",
    "Thus, $J(f) = \\begin{bmatrix}1 && 1 && 1 && \\dots && 1\\end{bmatrix}$ regardless of $x_1 \\dots x_n$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 3, requires_grad=True)\n",
    "Y = X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.backward()\n",
    "X.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following chain of operations on vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = a + b\n",
    "e = c.sum()\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\bm{e}}{\\partial \\bm{a}} = \\frac{\\partial \\bm{e}}{\\partial \\bm{c}}\\frac{\\partial \\bm{c}}{\\partial \\bm{a}} $$\n",
    "\n",
    "Important to note that the above (simple chain rule) only works because `a` and `b` themselves are not parameterized by another variable. Otherwise we'd need the multivariate law of [total derivatives](https://en.wikipedia.org/wiki/Total_derivative).\n",
    "\n",
    "Given what we know from above with gradients of elementwise operations & vector summation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dc = np.array([1, 1, 1])\n",
    "dc_db = np.eye(3)\n",
    "dc_da = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_da = de_dc @ dc_da\n",
    "de_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matmul is pointless; just to show for rigor.\n",
    "\n",
    "Again, comparing w/ PyTorch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "b = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "c = a + b\n",
    "e = c.sum()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the law of total derivatives mentioned above?\n",
    "\n",
    "We need this when we need to propagate differentials to output when there are \"shared\" inputs. For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float16)\n",
    "b = a ** 2\n",
    "c = a + b\n",
    "d = c.sum()\n",
    "\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5., 7.], dtype=torch.float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to know $\\frac{\\partial c}{\\partial a}$.\n",
    "\n",
    "With just knowing the single-variable chain rule, we would attempt to calculate this by computing...\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial a} = \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} $$\n",
    "\n",
    "This would be wrong, however, since $c$ actually depends on $a$ not only (indirectly) from $b$, but also directly from $a$.\n",
    "\n",
    "$$ a = a $$\n",
    "$$ b = a^2 $$\n",
    "$$ c = a + b $$\n",
    "$$ d = sum(c) $$\n",
    "\n",
    "We need to sum *all* sources of contributions of $a$ to $c$.\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial a} = \\frac{\\partial c}{\\partial a} + \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} $$\n",
    "\n",
    "To make things clearer, we can denote the $a$ from $a + b$ as $a_1$ and the $a$ from $a^2$ as $a_2$. Then, \n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial a} = \\frac{\\partial c}{\\partial a_1} + \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of these concepts, we can establish a more computationally efficient framework to compute VJPs--automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBackward:\n",
    "    def __init__(self, child1, child2):\n",
    "        self.child1 = child1\n",
    "        self.child2 = child2\n",
    "\n",
    "    def vjp(self, grad):\n",
    "        return grad, grad  # since we just matmul the grads with identity matrix, as seen earlier, we can just pass the grad along w/ no modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumBackward:\n",
    "    def __init__(self, child):\n",
    "        self.child = child\n",
    "\n",
    "    def vjp(self, grad):\n",
    "        return grad * np.ones_like(self.child), None  # effectively useless, but for rigor again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,\n",
    "                 value: np.array,\n",
    "                 grad_fn = None,\n",
    "                 children = (None, None)):\n",
    "        self.value = value\n",
    "        self.grad = np.zeros_like(self.value)\n",
    "        self.grad_fn = grad_fn\n",
    "        self.children = children\n",
    "        self.visited = False\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(value={self.value}, grad_fn={self.grad_fn})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Value(value=self.value + other.value,\n",
    "                     grad_fn=AddBackward(self.value, other.value),\n",
    "                     children=(self, other))\n",
    "\n",
    "    def sum(self):\n",
    "        return Value(value=np.sum(self.value),\n",
    "                     grad_fn=SumBackward(self.value),\n",
    "                     children=(self, None))\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        if not (self.value.ndim > 1):  # if we're a scalar value (i.e, f: R^n -> R)\n",
    "            if grad is None:\n",
    "                grad = np.array(1)  # kick off gradient chain\n",
    "        self.grad += grad\n",
    "        if self.grad_fn:\n",
    "            vjpL, vjpR = self.grad_fn.vjp(grad)\n",
    "            if vjpR is None:\n",
    "                self.children[0].backward(vjpL)\n",
    "            else:\n",
    "                self.children[0].backward(vjpL)\n",
    "                self.children[1].backward(vjpR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(np.array([1, 2, 3]))\n",
    "b = Value(np.array([4, 5, 6]))\n",
    "c = a + b  # [5, 7, 8]\n",
    "d = c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, everything's as we expect it to be. Let's add support for the Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulBackward:\n",
    "    def __init__(self, child1, child2):\n",
    "        self.child1 = child1\n",
    "        self.child2 = child2\n",
    "\n",
    "    def vjp(self, grad):\n",
    "        return (grad * self.child2, grad * self.child1)  # from our earlier observation that if z = x * y, the gradient of z w.r.t x = diag(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,\n",
    "                 value: np.array,\n",
    "                 grad_fn = None,\n",
    "                 children = (None, None)):\n",
    "        self.value = value\n",
    "        self.grad = np.zeros_like(self.value)\n",
    "        self.grad_fn = grad_fn\n",
    "        self.children = children\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(value={self.value}, grad_fn={self.grad_fn})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Value(value=self.value + other.value,\n",
    "                     grad_fn=AddBackward(self.value, other.value),\n",
    "                     children=(self, other))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(value=self.value * other.value,\n",
    "                     grad_fn=MulBackward(self.value, other.value),\n",
    "                     children=(self, other))\n",
    "\n",
    "    def sum(self):\n",
    "        return Value(value=np.sum(self.value),\n",
    "                     grad_fn=SumBackward(self.value),\n",
    "                     children=(self, None))\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        if not (self.value.ndim > 1):  # if we're a scalar value (i.e, f: R^n -> R)\n",
    "            if grad is None:\n",
    "                grad = np.array(1)  # kick off gradient chain\n",
    "        self.grad += grad\n",
    "        if self.grad_fn:\n",
    "            vjpL, vjpR = self.grad_fn.vjp(grad)\n",
    "            if vjpR is None:\n",
    "                self.children[0].backward(vjpL)\n",
    "            else:\n",
    "                self.children[0].backward(vjpL)\n",
    "                self.children[1].backward(vjpR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it on a larger chain of expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(np.array([1, 2, 3]))\n",
    "b = Value(np.array([4, 5, 6]))\n",
    "c = a * b\n",
    "\n",
    "d = Value(np.array([7, 8, 9]))\n",
    "e = Value(np.array([10, 11, 12]))\n",
    "f = d + e\n",
    "\n",
    "g = c * f\n",
    "h = g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4, 10, 18]),\n",
       " Value(value=[ 4 10 18], grad_fn=<__main__.MulBackward object at 0x13715ba10>))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.grad, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f.grad` is what we expect, since it's equal to `c`, which is $[1(4), 2(5), 3(6)]$. Likewise, `c.grad` should be equal to `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17, 19, 21]),\n",
       " Value(value=[17 19 21], grad_fn=<__main__.AddBackward object at 0x1371235f0>))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big issue with the above implementation is that while it's mathematically correct (at least for the three simple operators seen above), parallelizing these operations properly on a GPU isn't easy.\n",
    "\n",
    "This automatic differentiation framework effectively creates a large computational DAG, and we need to parallelize a traversal of this graph. We also can't rely on a recursive implementation of passing `grad` around as seen with our current `backward()` method. \n",
    "\n",
    "This was the pain point of my earlier [project](https://github.com/Parxd/accelerate), as although I used CuPy as a drop-in replacement for NumPy to try and at least parallelize the pure computations, it couldn't handle a parallel traversal of the actual operation graph, making it very slow and inefficient.\n",
    "\n",
    "*This* [project](https://github.com/Parxd/cudaML) is an attempt to fix these pain points by using CUDA itself rather than a Python wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
